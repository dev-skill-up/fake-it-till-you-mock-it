{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fake It 'Till You Mock It\n",
    "\n",
    "Welcome to the Mock Tutorial notebook. This will be the resource we use to both learn, and test what we learned. This notebook is designed to be loaded to a Jupyter Lab instance that has `pytest` and `ipytest` installed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE** I hope you had a chance to go through the preparation notebook and validate your environment. If you have not done so, and you have any problems, please use the time in the first Hands-On section to run through that notebook. It has validation instructions as well as some useful troubleshooting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a new virtual environment, do\n",
    "\n",
    "```\n",
    "$ pip install pytest ipytest jupyterlab\n",
    "```\n",
    "\n",
    "(If you were in the previous tutorial, we are using a different pytest/Jupyter integration,\n",
    "after some issues with the previous one.)\n",
    "\n",
    "When this is done, launch Jupyter\n",
    "\n",
    "```\n",
    "$ jupyter lab\n",
    "```\n",
    "\n",
    "Click on the upload icon, and upload this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step will be to load the `ipytest` Jupyter extension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipytest\n",
    "ipytest.autoconfig()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There should not be any output from this step. If an error occured saying \"module not found\", make sure the virtual environment has `ipytest` installed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recap\n",
    "\n",
    "This section covers quickly the basics of unit tests, pytests, and mocks.\n",
    "\n",
    "* Unit tests: functions that exercise production code, usually isolated from environment.\n",
    "* pytest: a unit test *runner*.\n",
    "* mocks: objects used instead of \"real objects\" in production code which interact with environment.\n",
    "\n",
    "This tutorial uses Jupyter to run tests.\n",
    "While this is different from how tests run in \"real life\"\n",
    "(usually via Continuous Integration, or locally via tools like `tox`),\n",
    "it is a good way to learn how to write tests.\n",
    "Pytest is the most popular test runner, so will learn to write unit tests that take advantage of pytest.\n",
    "\n",
    "Unit tests are especially useful in making sure your code works under *extreme conditions*. Does your certificate rotation code do the right thing when the file system is full? Does your web service work when the connection to the backend disconnects a lot? How does your data analysis code handle corrupted inputs?\n",
    "\n",
    "Those are things that are sometimes hard to test in real systems, but are *exactly* the things that make the difference between high-quality code versus finding yourself in the incident review meeting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start by running a simple test, that will mostly check that our environment is properly configured:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%run_pytest[clean]\n",
    "\n",
    "from unittest import mock\n",
    "import pytest\n",
    "\n",
    "@pytest.mark.parametrize('value', [1, 2])\n",
    "def test_something(value):\n",
    "    thing = mock.MagicMock()\n",
    "    thing.return_value = value\n",
    "    assert thing() == value + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This test had a few things that will be in a lot of our examples:\n",
    "\n",
    "* Creating a mock\n",
    "* Parametrizing a test\n",
    "* Failing tests intentionally to see how the assertion errors look like.\n",
    "\n",
    "Parametrizing tests is useful to run the *same test* with *different parameters*.\n",
    "This is useful, for example, to check that your code works with both positive and negative numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**HANDS ON** \n",
    "\n",
    "This is our first hands-on exercise. The next few will be a little more challenging, but this one is just to make sure we have our environment set up. We will have 10 minutes to finish it. We will get back to the tutorial at 16:50 (US/Pacific).\n",
    "\n",
    "Try to run it in your environment: go into the notebook, and\n",
    "\n",
    "* Execute the `import ipytest` cell, by clicking in and pressing Shift-Enter\n",
    "* Execute the `run_pytest` cell, by clicking in and pressing Shift-Enter.\n",
    "\n",
    "You should get a failure that looks much like the following one:\n",
    "\n",
    "```\n",
    "FF                                                                          [100%]\n",
    "==================================== FAILURES =====================================\n",
    "________________________________ test_something[1] ________________________________\n",
    "\n",
    "value = 1\n",
    "\n",
    "    @pytest.mark.parametrize('value', [1, 2])\n",
    "    def test_something(value):\n",
    "        thing = mock.MagicMock()\n",
    "        thing.return_value = value\n",
    ">       assert thing() == value + 1\n",
    "E       AssertionError: assert 1 == (1 + 1)\n",
    "E        +  where 1 = <MagicMock id='139863682412256'>()\n",
    "\n",
    "<ipython-input-24-628b0c93adc5>:8: AssertionError\n",
    "________________________________ test_something[2] ________________________________\n",
    "\n",
    "value = 2\n",
    "\n",
    "    @pytest.mark.parametrize('value', [1, 2])\n",
    "    def test_something(value):\n",
    "        thing = mock.MagicMock()\n",
    "        thing.return_value = value\n",
    ">       assert thing() == value + 1\n",
    "E       AssertionError: assert 2 == (2 + 1)\n",
    "E        +  where 2 = <MagicMock id='139863674357552'>()\n",
    "\n",
    "<ipython-input-24-628b0c93adc5>:8: AssertionError\n",
    "============================= short test summary info =============================\n",
    "FAILED tmp46_xden7.py::test_something[1] - AssertionError: assert 1 == (1 + 1)\n",
    "FAILED tmp46_xden7.py::test_something[2] - AssertionError: assert 2 == (2 + 1)\n",
    "2 failed in 0.03s\n",
    "```\n",
    "\n",
    "If you did not, then something might not be installed and configured properly. Check again that `pytest` and `ipytest` are properly installed in your virtual environment.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mocking Recap (16:50 US/Pacific)\n",
    "\n",
    "Sometimes, using real objects is hard, ill-advised, or complicated.\n",
    "\n",
    "For example, a `requests.Session` connects to real websites:\n",
    "using it in your unittests invites a...lot...of problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unittest import mock"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Mocks\" are a unittest concept: they produce objects that are substitutes for the real ones.\n",
    "There's a whole cottage industry that will explain that \"mock\", \"fake\", and \"stub\" are all subtly different.\n",
    "We'll use all of those interchangably."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regular = mock.MagicMock()\n",
    "\n",
    "def do_something(o):\n",
    "    return o.something(5)\n",
    "\n",
    "do_something(regular)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mocks have \"all the methods\". The methods will usually return another Mock.\n",
    "This can be changed by assigning to `return_value`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%run_pytest[clean]\n",
    "\n",
    "def do_something(o):\n",
    "    return o.something() + 1\n",
    "\n",
    "def test_something():\n",
    "    obj = mock.MagicMock(name=\"an object\")\n",
    "    obj.something.return_value = 2\n",
    "    assert do_something(obj) == 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also possible to override the \"magic methods\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%run_pytest[clean]\n",
    "\n",
    "from unittest import mock\n",
    "\n",
    "def test_magic():\n",
    "    a = mock.MagicMock()\n",
    "    a.__str__.return_value = \"an a\"\n",
    "    assert str(a) == \"a\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also make sure that a mock does not have \"extra\" methods or attributes by using a spec."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%run_pytest[clean]\n",
    "\n",
    "import httpx\n",
    "\n",
    "def bad_http_code(client):\n",
    "    ## TYPO: psot instead of post\n",
    "    client.psot(\"https://example.com/upload-data\", json=dict(a=1))\n",
    "\n",
    "from unittest import mock\n",
    "\n",
    "def test_bad_http_code():\n",
    "    dummy_client = mock.MagicMock(spec=httpx.Client)\n",
    "    bad_http_code(dummy_client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**HANDS ON**: Fix this test! Only change the marked line to get the test to pass. We will have 5 minutes for this exercise. We will reconvene at 17:00 (US/Pacific)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%run_pytest[clean]\n",
    "\n",
    "from unittest import mock\n",
    "\n",
    "def analyze_website(client):\n",
    "    ret = client.get(\"http://example.com\").text\n",
    "    return ret[10:15]\n",
    "\n",
    "def test_analyze():\n",
    "    client = mock.MagicMock(name=\"client\", spec=httpx.Client)\n",
    "    pass # fix this line\n",
    "    assert analyze_website(client) == \"hello\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mock Side Effect: More Interesting Behavior (17:00 US/Pacific)\n",
    "\n",
    "Sometimes, having a `MagicMock` that returns the same thing every time doesn't cut it.\n",
    "For example, we expect `sys.stdin.readline()` to return different values,\n",
    "not the same value throughout the test.\n",
    "\n",
    "The property `side_effect` allows controlling what a magic mock returns on a finer-grain level\n",
    "than using `return_value`. \n",
    "\n",
    "### Iterable\n",
    "\n",
    "One of the things that can be assigned to `side_effect` is\n",
    "an *iterable*, such as a sequence or a generator.\n",
    "\n",
    "This is a powerful feature -- it allows controlling each call's return value,\n",
    "with little code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%run_pytest[clean]\n",
    "\n",
    "from unittest import mock\n",
    "\n",
    "def test_values():\n",
    "    different_things = mock.MagicMock()\n",
    "    different_things.side_effect = [1, 2, 3]\n",
    "    assert different_things() == 1\n",
    "    assert different_things() == 2\n",
    "    assert different_things() == 4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A more realistic example is when simulating file input.\n",
    "In this case, we want to be able to control what `readline` returns\n",
    "each time to pretend it is file input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%run_pytest[clean]\n",
    "\n",
    "from unittest import mock\n",
    "\n",
    "def parse_three_lines(fpin):\n",
    "    line = fpin.readline()\n",
    "    name, value = line.split()\n",
    "    modifier = fpin.readline().strip()\n",
    "    extra = fpin.readline().strip()\n",
    "    return {name: f\"{value}/{modifier}+{extra}\"}\n",
    "\n",
    "from io import TextIOBase\n",
    "    \n",
    "def test_parser():\n",
    "    filelike = mock.MagicMock(spec=TextIOBase)\n",
    "    filelike.readline.side_effect = [\n",
    "        \"thing important\\n\",\n",
    "        \"a-little\\n\",\n",
    "        \"to-some-people\\n\"\n",
    "    ]\n",
    "    value = parse_three_lines(filelike)\n",
    "    assert value == dict(thing=\"important/a-little+to-most-people\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**HANDS ON** Fix the following cell. Only change the line marked. We will have 10 minutes for this exercise, and reconvene at 17:20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%run_pytest[clean]\n",
    "\n",
    "def read_markdown_header(fpin):\n",
    "    ret_value = {}\n",
    "    line = fpin.readline()\n",
    "    if line != \"---\\n\":\n",
    "        raise ValueError(\"invalid\", line)\n",
    "    for i in range(100):\n",
    "        line = fpin.readline()\n",
    "        if line == \"---\\n\":\n",
    "            break\n",
    "        name, value = line.split(\": \")\n",
    "        ret_value[name] = value ## Only change this line\n",
    "    return ret_value\n",
    "\n",
    "from io import TextIOBase\n",
    "from unittest import mock\n",
    "\n",
    "def test_parser():\n",
    "    filelike = mock.MagicMock(spec=TextIOBase)\n",
    "    filelike.readline.side_effect = [\n",
    "        \"---\\n\",\n",
    "        \"title: Name of Post\\n\",\n",
    "        \"author: Some One\\n\",\n",
    "        \"---\\n\"\n",
    "    ]\n",
    "    value = read_markdown_header(filelike)\n",
    "    assert value == dict(title=\"Name of Post\", author=\"Some One\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exception\n",
    "\n",
    "Another thing that is possible to do is to assign an *exception* to the `side_effect` attribute.\n",
    "This will cause the call to raise this exception.\n",
    "Using this feature allows simulating edge conditions in the environment:\n",
    "often precisely the ones that\n",
    "\n",
    "* You care about\n",
    "* Are hard to simulate realistically\n",
    "\n",
    "One popular case is network issues: as per Murphy's law, they will always happen at 4am causing a pager to go off,\n",
    "and never at 10am when you are seating at your desk. The following is based on real code I wrote to test a network service.e \n",
    "\n",
    "In this simplified example, the code returns the length of the response line, or a negative number if a timeout has been reached: the number is different based on when in the protocol negotiation this has been reached. This allows the code \n",
    "to distinguish \"connection timeout\" from \"response timeout\", for example. Testing this code against a real server is hard: servers try hard to avoid outages! You could fork the server C code and add some \"chaos\" or...just use `side_effect` and mock."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%run_pytest[clean]\n",
    "import socket\n",
    "\n",
    "def careful_reader(sock):\n",
    "    sock.settimeout(5)\n",
    "    try:\n",
    "        sock.connect((\"some.host\", 8451))\n",
    "    except socket.timeout:\n",
    "        return -1\n",
    "    try:\n",
    "        sock.sendall(b\"DO THING\\n\")\n",
    "    except socket.timeout:\n",
    "        return -2\n",
    "    fpin = sock.makefile()\n",
    "    try:\n",
    "        line = fpin.readline()\n",
    "    except socket.timeout:\n",
    "        return -3\n",
    "    return len(line.strip())\n",
    "\n",
    "from io import TextIOBase\n",
    "from unittest import mock\n",
    "\n",
    "def test_reader():\n",
    "    sock = mock.MagicMock(spec=socket.socket)\n",
    "    sock.connect.side_effect = socket.timeout(\"too long\")\n",
    "    assert careful_reader(sock) == 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**HANDS ON** Get all tests to work. Only change the lines marked. Because this exercise is a bit more subtle, you get one test\n",
    "that already works for free. Now you only have to get the others to pass! We will have 10 minutes for this exercise. We will reconvene at 17:35."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%run_pytest[clean]\n",
    "import socket\n",
    "\n",
    "def careful_reader(sock):\n",
    "    sock.settimeout(5)\n",
    "    try:\n",
    "        sock.connect((\"some.host\", 8451))\n",
    "    except socket.timeout:\n",
    "        return -1\n",
    "    try:\n",
    "        sock.sendall(b\"DO THING\\n\")\n",
    "    except socket.timeout:\n",
    "        return -2\n",
    "    fpin = sock.makefile()\n",
    "    print(fpin.readline.side_effect)\n",
    "    try:\n",
    "        line = fpin.readline()\n",
    "    except socket.timeout:\n",
    "        return -3\n",
    "    return len(line.strip())\n",
    "\n",
    "from io import TextIOBase\n",
    "from unittest import mock\n",
    "\n",
    "def test_reader_1():\n",
    "    # This test works\n",
    "    sock = mock.MagicMock(spec=socket.socket)\n",
    "    sock.connect.side_effect = socket.timeout(\"too long\")\n",
    "    assert careful_reader(sock) == -1\n",
    "    \n",
    "def test_reader_2():\n",
    "    sock = mock.MagicMock(spec=socket.socket)\n",
    "    # Add only one line\n",
    "    assert careful_reader(sock) == -2\n",
    "    \n",
    "def test_reader_3():\n",
    "    sock = mock.MagicMock(spec=socket.socket)\n",
    "    # Add only one line\n",
    "    assert careful_reader(sock) == -3\n",
    "\n",
    "def test_reader_pos():\n",
    "    sock = mock.MagicMock(spec=socket.socket)\n",
    "    # Add only one line\n",
    "    assert careful_reader(sock) == 10\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BREAK**\n",
    "\n",
    "We are going to take a break. Get food, stretch your muscles, relex. We will get back in 10 minutes, at 17:45 US/Pacific"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Callable (17:45 US/Pacific)\n",
    "\n",
    "As mentioned, the above example was simplified: real network service test code should verify that the results it got were correct to validate that the server works correctly. This means doing a synthetic request and looking for a correct result. The mock object has to emulate that. It has to perform some computation on the inputs.\n",
    "\n",
    "Trying to test such code without performing any computation is difficult. The tests tend to be too *insensitive* or too *flakey*. An insensitive test is one that does not fail in the presence of bugs. A flakey test is one that sometimes fails, even when the code is  correct. Here, our code is incorrect. The insensitive test does not catch it, while the flakey test would fail even if it was fixed!\n",
    "\n",
    "(This code is loosely based on a memcache checker I built for work.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%run_pytest[clean]\n",
    "import socket\n",
    "import random\n",
    "\n",
    "def yolo_reader(sock):\n",
    "    sock.settimeout(5)\n",
    "    sock.connect((\"some.host\", 8451))\n",
    "    fpin = sock.makefile()\n",
    "    order = [0, 1]\n",
    "    random.shuffle(order)\n",
    "    while order:\n",
    "        if order.pop() == 0:\n",
    "            sock.sendall(b\"GET KEY\\n\")\n",
    "            key = fpin.readline().strip()\n",
    "        else:\n",
    "            sock.sendall(b\"GET VALUE\\n\")\n",
    "            value = fpin.readline().strip()\n",
    "    return {value: key} ## Woops bug, should be {key: value}\n",
    "    \n",
    "from io import TextIOBase\n",
    "from unittest import mock\n",
    "import pytest\n",
    "\n",
    "def test_insensitive_test():\n",
    "    sock = mock.MagicMock(spec=socket.socket)\n",
    "    sock.makefile.return_value.readline.return_value = \"interesting\\n\"\n",
    "    assert yolo_reader(sock) == {\"interesting\": \"interesting\"}\n",
    "    \n",
    "@pytest.mark.parametrize(\"does_nothing\", [1, 2, 3, 4, 5])\n",
    "def test_flakey_test(does_nothing):\n",
    "    sock = mock.MagicMock(spec=socket.socket)\n",
    "    sock.makefile.return_value.readline.side_effect = [\"key\\n\", \"value\\n\"]\n",
    "    assert yolo_reader(sock) == {\"key\": \"value\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final option of getting results from a mock object is to assign a *callable object* to `side_effect`. This calls `side_effect` to simply call it. Why not just assign a callable object directly to the attribute? Have patience, we'll get to that in the next part!\n",
    "\n",
    "In this example, our callable object (just a function) will assign a `return_value` to the attribute of another object. This is not that uncommon. We are simulating the environment, and in a real environment, poking one thing often has an effect on other things."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%run_pytest[clean]\n",
    "import socket\n",
    "import random\n",
    "\n",
    "def yolo_reader(sock):\n",
    "    sock.settimeout(5)\n",
    "    sock.connect((\"some.host\", 8451))\n",
    "    fpin = sock.makefile()\n",
    "    order = [0, 1]\n",
    "    random.shuffle(order)\n",
    "    while order:\n",
    "        if order.pop() == 0:\n",
    "            sock.sendall(b\"GET KEY\\n\")\n",
    "            key = fpin.readline().strip()\n",
    "        else:\n",
    "            sock.sendall(b\"GET VALUE\\n\")\n",
    "            value = fpin.readline().strip()\n",
    "    return {key: value} ## Woops bug, should be {key: value}\n",
    "    \n",
    "from io import TextIOBase\n",
    "from unittest import mock\n",
    "\n",
    "def test_yolo_well():\n",
    "    sock = mock.MagicMock(spec=socket.socket)\n",
    "    def sendall(data):\n",
    "        cmd, name = data.decode(\"ascii\").split()\n",
    "        if name == \"KEY\":\n",
    "            sock.makefile.return_value.readline.return_value = \"key\\n\"\n",
    "        elif name == \"VALUE\":\n",
    "            sock.makefile.return_value.readline.return_value = \"value\\n\"\n",
    "        else:\n",
    "            raise ValueError(\"got bad command\", name)\n",
    "    sock.sendall.side_effect = sendall\n",
    "    assert yolo_reader(sock) == {\"key\": \"value\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**HANDS ON** Fix this test! This is not an easy exercise, but it will teach you about how mock objects work. We will have 10 minutes for this exercise. We will reconvene at 18:00"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%run_pytest[clean]\n",
    "import socket\n",
    "import random\n",
    "\n",
    "def echo_computer(sock, num):\n",
    "    sock.settimeout(5)\n",
    "    sock.connect((\"some.host\", 8451))\n",
    "    fpin = sock.makefile()\n",
    "    value = random.randint(0, 4)\n",
    "    sock.sendall(f\"ECHO {value}\\n\".encode(\"ascii\"))\n",
    "    response = fpin.readline().strip()\n",
    "    if response != str(value):\n",
    "        return 0\n",
    "    return 5 * num\n",
    "   \n",
    "from io import TextIOBase\n",
    "from unittest import mock\n",
    "\n",
    "def test_echo_computer():\n",
    "    sock = mock.MagicMock(spec=socket.socket)\n",
    "    fakefile = sock.makefile.return_value\n",
    "    def sendall(data):\n",
    "        pass # Change only this line\n",
    "    sock.sendall.side_effect = sendall\n",
    "    assert echo_computer(sock, 2) == 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mock call arguments: X-Ray for Code (18:00 US/Pacific)\n",
    "\n",
    "When writing a unit test, you are \"away\" from the code, but trying to peer into its guts to see how it behaves.\n",
    "The Mock object is your sneaky spy. After it gets into the production code, it records everything faithfully.\n",
    "This is how you can find what your code does, and whether it is the right thing.\n",
    "\n",
    "### Call counts\n",
    "\n",
    "The simplest thing is to just make sure that the code is called the expected number of times.\n",
    "The `.call_count` attribute is exactly what counts that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%run_pytest[clean]\n",
    "\n",
    "def get_values(names, client):\n",
    "    ret_value = []\n",
    "    cache = {}\n",
    "    for name in names:\n",
    "        if name not in cache:\n",
    "            value = client.get(f\"https://httpbin.org/anything/grab?name={name}\").json()['args']['name']\n",
    "            cache[name] = value\n",
    "        ret_value.append(cache[name])\n",
    "    return ret_value\n",
    "\n",
    "def test_get_values():\n",
    "    client = mock.MagicMock()\n",
    "    client.get.return_value.json.return_value = dict(args=dict(name=\"something\"))\n",
    "    result = get_values(['one', 'One'], client)\n",
    "    assert result == ['something', 'something']\n",
    "    assert client.get.call_count == 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One benefit of checking `.call_count >= 1` as opposed to checking `.called` is that it is more resistant to silly typos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%run_pytest[clean]\n",
    "\n",
    "def call_function(func):\n",
    "    print(\"I'm going to call the function, really!\")\n",
    "    if False:\n",
    "        func()\n",
    "    print(\"I just called the function\")\n",
    "\n",
    "def test_call_function():\n",
    "    # This test passes!\n",
    "    func = mock.MagicMock()\n",
    "    call_function(func)\n",
    "    assert func.callled # TYPO -- Extra \"l\"\n",
    "    \n",
    "def test_call_function_carefully():\n",
    "    # This test fails because it has a bug.\n",
    "    func = mock.MagicMock()\n",
    "    call_function(func)\n",
    "    assert func.calll_count >= 1 # TYPO -- Extra \"l\"\n",
    "\n",
    "def test_call_function_carefully_and_correctly():\n",
    "    # This test fails because the function has a bug.\n",
    "    func = mock.MagicMock()\n",
    "    call_function(func)\n",
    "    assert func.call_count >= 1 # We finally managed to not have a typo\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using `spec` diligently can prevent that. However, `spec` is not recursive. Even if the original mock object has a spec, rare is the test that makes sure that every single attribute it has *also* has a spec. However, using `.call_count` instead of `.called` is a simple hack that will completely eliminate the chance to make this error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Call arguments\n",
    "\n",
    "In the following example, we want to make sure the code calls the method with the *correct* arguments.\n",
    "When automating data center manipulations, it is important to get things right.\n",
    "As they say, \"To err is human, but to destroy an entire data center requires a robot with a bug.\"\n",
    "\n",
    "We want to make sure our Paramiko-based automation will correctly get the sizes of files, even when the file names have spaces in them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%run_pytest[clean]\n",
    "\n",
    "def get_remote_file_size(client, fname):\n",
    "    client.connect('ssh.example.com')\n",
    "    stdin, stdout, stderr = client.exec_command(f\"ls -l {fname}\")\n",
    "    stdin.close()\n",
    "    results = stdout.read()\n",
    "    errors = stderr.read()\n",
    "    stdout.close()\n",
    "    stderr.close()\n",
    "    if errors != '':\n",
    "        raise ValueError(\"problem with command\", errors)\n",
    "    return int(results.split()[4])\n",
    "\n",
    "import pytest\n",
    "from unittest import mock\n",
    "import shlex\n",
    "\n",
    "@pytest.mark.parametrize(\"fname\", [\"readme.txt\", \"a file\"])\n",
    "def test_file_size(fname):\n",
    "    client = mock.MagicMock()\n",
    "    client.exec_command.return_value = [mock.MagicMock(name=str(i)) for i in range(3)]\n",
    "    client.exec_command.return_value[1].read.return_value = f\"\"\"\\\n",
    "    -rw-rw-r--  1 user user    123 Jul 18 20:25 {fname}\n",
    "    \"\"\"\n",
    "    client.exec_command.return_value[2].read.return_value = \"\"\n",
    "    result = get_remote_file_size(client, fname)\n",
    "    assert result == 123\n",
    "    [args], kwargs = client.exec_command.call_args\n",
    "    assert shlex.split(args) == [\"ls\", \"-l\", fname]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# List of call args\n",
    "\n",
    "Sometimes, this is not enough. Some code calls functions repeatedly, and we need to test that *all* calls are correct.\n",
    "The most sophisticated X-Ray we have is `.call_args_list` which gives the entire history of what happened to the callable.\n",
    "\n",
    "For this example, we will pretend that the (*real*) remote calculator API only allows multiplying two numbers. In order to *cube* the number, calculate `x**3`, we need two calls to the service. For superstitious reasons, we want to always put the bigger number first: maybe someone told us that it is faster this way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%run_pytest[clean]\n",
    "\n",
    "import httpx\n",
    "from unittest import mock\n",
    "\n",
    "def calculate_cube(client, base):\n",
    "    square = int(client.get(f\"https://api.mathjs.org/v4/?expr={base}*{base}\").text) # x*x\n",
    "    return int(client.get(f\"https://api.mathjs.org/v4/?expr={base}*{square}\").text) # x*x*x\n",
    "\n",
    "def test_calculate_cube():\n",
    "    client = mock.MagicMock(spec=httpx.Client)\n",
    "    client.get.side_effect = [mock.MagicMock(text=str(x)) for x in [25, 125]]\n",
    "    assert calculate_cube(client, 5) == 125\n",
    "    assert client.get.call_count == 2\n",
    "    squaring, cubing = client.get.call_args_list\n",
    "    args, kwargs = squaring\n",
    "    assert kwargs == {}\n",
    "    assert args == tuple([\"https://api.mathjs.org/v4/?expr=5*5\"])\n",
    "    args, kwargs = cubing\n",
    "    assert kwargs == {}\n",
    "    ## Make sure bigger number comes first!\n",
    "    assert args == tuple([\"https://api.mathjs.org/v4/?expr=25*5\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**HANDS ON** The code uses random, and currently the test is flakey. Fix it so that the test always passes. It is parametrized to run five times so that there is only 1/32 chance that it will pass if it does not do the right thing. We will have 5 minutes for this exercise, and reconvene at 18:20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%run_pytest[clean]\n",
    "\n",
    "import httpx\n",
    "from unittest import mock\n",
    "import pytest\n",
    "import random\n",
    "\n",
    "def calculate_fifth_power(client, base):\n",
    "    square = int(client.get(f\"https://api.mathjs.org/v4/?expr={base}*{base}\").text)\n",
    "    hypercube = int(client.get(f\"https://api.mathjs.org/v4/?expr={square}*{square}\").text)\n",
    "    # random order\n",
    "    args = [hypercube, base]\n",
    "    random.shuffle(args)\n",
    "    result = int(client.get(f\"https://api.mathjs.org/v4/?expr={args[0]}*{args[1]}\").text)\n",
    "    return result\n",
    "\n",
    "@pytest.mark.parametrize(\"does_nothing\", [1, 2, 3, 4, 5])\n",
    "def test_calculate_cube(does_nothing):\n",
    "    client = mock.MagicMock(spec=httpx.Client)\n",
    "    client.get.side_effect = [mock.MagicMock(text=str(x)) for x in [25, 625, 3125]]\n",
    "    assert calculate_fifth_power(client, 5) == 3125\n",
    "    assert client.get.call_count == 3\n",
    "    squaring, hypercubing, final = client.get.call_args_list\n",
    "    args, kwargs = squaring\n",
    "    assert kwargs == {}\n",
    "    assert args == tuple([\"https://api.mathjs.org/v4/?expr=5*5\"])\n",
    "    args, kwargs = hypercubing\n",
    "    assert kwargs == {}\n",
    "    args, kwargs = final\n",
    "    assert kwargs == {}\n",
    "    [url] = args\n",
    "    constant, expr = url.split(\"=\", 1)\n",
    "    assert constant == \"https://api.mathjs.org/v4/?expr\"\n",
    "    assert expr == \"625*5\" # Change only this line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recap: Deep Dive into Mocks (18:20 US/Pacific)\n",
    "\n",
    "Mocks have a *lot* of power. Like any powerful tool, using it properly is a fast way to get into a big mess. But properly using the `.return_value`, `.side_effect`, and the various `.call*` properties, it is possible to write the best sort of unit tests. A good unit test is one that\n",
    "\n",
    "* Will fail in the presence of incorrect code\n",
    "* Will pass in the presence of correct code\n",
    "\n",
    "\"Quality\" is not binary. It exists on a spectrum. The *badness* of a unit test will be determined by:\n",
    "\n",
    "* How many errors it lets pass (\"missing alarms\" AKA \"false negatives\" or, if you are a statistician, \"type 2 errors\")\n",
    "* How many *correct* code changes it fails (\"false alarms\" AKA \"false positives\" or, if you a statistician, \"type 1 errors\")\n",
    "\n",
    "When using a mocks, take the time and think about both metrics to evaluate whether this mock, and this unittest, will help or hinder you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mock-tutorial",
   "language": "python",
   "name": "mock-tutorial-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
